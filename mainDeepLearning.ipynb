{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f179d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_folder_path = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3be20",
   "metadata": {},
   "source": [
    "## Loading the training dataset\n",
    "The numpy array has the following shapes :\n",
    "- 6556 rows, reprensenting 6556 event occurences\n",
    "- 1445 columns representing :  \n",
    "-- 1 column of step value (initialized for every serie id (ie test subject))  \n",
    "-- 721 columns of **anglez** samples, centered 30 minutes before and after an event  \n",
    "-- 721 columns of **enmo** samples, centered 30 minutes before and after an event  \n",
    "-- 1 column of timestamp, ie the moment of the sample   \n",
    "-- 1 column of **event** which is the **target** of the dataset. (0: *onset*, 1: *wakeup*)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a1a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000e+00 0.00000e+00 0.00000e+00 ... 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00]\n",
      " [2.18420e+01 2.85359e+01 3.44465e+01 ... 4.70000e-03 4.70000e-03\n",
      "  4.60000e-03]]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "offset = 75\n",
    "data = np.load(f\"{current_folder_path}/training_data/data_{offset}.npy\")\n",
    "\n",
    "#[step, :, :, timestamp, event]\n",
    "step = data[:,0]\n",
    "X = data[:, 1:-2]\n",
    "Y = data[:, -1]\n",
    "\n",
    "print(X[0:2, :])\n",
    "print(Y[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f89df0",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d970b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4917, 1442]) torch.Size([4917, 1]) torch.Size([1639, 1442]) torch.Size([1639, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "train_features = torch.from_numpy(X_train).float()\n",
    "train_labels = torch.from_numpy(Y_train).float().unsqueeze(1)\n",
    "test_features = torch.from_numpy(X_test).float()\n",
    "test_labels = torch.from_numpy(Y_test).float().unsqueeze(1)\n",
    "\n",
    "train_features = F.normalize(train_features)\n",
    "test_features = F.normalize(test_features)\n",
    "\n",
    "print(train_features.size(), train_labels.size(), test_features.size(), test_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e734e6",
   "metadata": {},
   "source": [
    "## NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68401c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.layer1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.layer2 = nn.Linear(hidden_size//2, hidden_size//4)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size//2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size//4)\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size//4, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()  \n",
    "        self.dropout = nn.Dropout1d(p=0.1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.output_layer(x)  \n",
    "        #x = self.relu(x)    \n",
    "        x = self.sigmoid(x)\n",
    "        return x        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d15b21",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49a22942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 48.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 34.71864132583141\n",
      "[0.756949 0.580346 0.560412 0.548129 0.536322 0.524146 0.517991 0.507865\n",
      " 0.499804 0.495006 0.484265 0.476457 0.468436 0.46367  0.454037 0.452745\n",
      " 0.438198 0.428868 0.422587 0.410308 0.40088  0.384408 0.368587 0.358067\n",
      " 0.349259 0.334542 0.324217 0.313738 0.289402 0.285854 0.273766 0.264003\n",
      " 0.255959 0.246959 0.242625 0.229477 0.226257 0.213829 0.20641  0.20045\n",
      " 0.206041 0.189233 0.187891 0.186613 0.17468  0.172032 0.171065 0.164134\n",
      " 0.157074 0.155987]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m, n = train_features.shape\n",
    "input_size = n\n",
    "print(input_size)\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimize = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "model.train() \n",
    "running_loss = 0.0\n",
    "\n",
    "losses_list = []\n",
    "num_epochs = int(input(\"Number of epochs : \"))    \n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_features)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    losses_list.append(loss.item())\n",
    "\n",
    "print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "print(np.round(losses_list[::2], 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f04eca",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d170cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2831, 0.9855, 0.9718, 0.9463, 0.9850, 0.0416, 0.9175, 0.6917, 0.0509,\n",
      "         0.1022]])\n",
      "[[0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]] [0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "1639 1460\n",
      "Accuracy on test set: 89.08%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_features) \n",
    "    #print(outputs[0:10].T)\n",
    "    #predicted = torch.round(outputs.data)\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    predicted = torch.round(outputs.data)\n",
    "    # predicted = np.round(outputs.numpy())\n",
    "    total += test_labels.size(0)\n",
    "    # correct += (predicted.numpy() == test_labels.numpy().T).sum().item()\n",
    "    correct += (predicted == test_labels).sum().item()        \n",
    "    \n",
    "    #print(predicted.numpy()[0:10].T, test_labels.numpy().T[0, 0:10])\n",
    "    #correct += (predicted == test_labels.numpy().T[0]).sum().item()\n",
    "    print(total, correct)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e45b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
